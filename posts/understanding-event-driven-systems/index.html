<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Saswata Chakravarty"><meta name=description content="https://saswatac.github.io"><meta name=keywords content="blog,developer,personal,ai,machine learning"><meta property="og:site_name" content="Saswata Chakravarty"><meta property="og:title" content="
  Understanding Event Driven Systems - Saswata Chakravarty
"><meta property="og:description" content="Learn how event driven asynchronous non blocking i/o works with the help of simple python code snippets."><meta property="og:type" content="website"><meta property="og:url" content="https://saswatac.github.io/posts/understanding-event-driven-systems/"><meta property="og:image" content="https://saswatac.github.io"><meta name=twitter:card content="summary"><meta name=twitter:site content="https://saswatac.github.io/posts/understanding-event-driven-systems/"><meta name=twitter:image content="https://saswatac.github.io"><base href=https://saswatac.github.io/posts/understanding-event-driven-systems/><title>Understanding Event Driven Systems - Saswata Chakravarty</title><link rel=canonical href=https://saswatac.github.io/posts/understanding-event-driven-systems/><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700"><link rel=stylesheet href=/css/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=alternate href=https://saswatac.github.io/index.xml type=application/rss+xml title="Saswata Chakravarty"><link href=https://saswatac.github.io/index.xml rel=feed type=application/rss+xml title="Saswata Chakravarty"><meta name=generator content="Hugo 0.83.1"></head><body><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Saswata Chakravarty</a>
<input type=checkbox id=menu-control>
<label class="menu-mobile float-right" for=menu-control><span class="btn-mobile float-right">&#9776;</span><ul class=navigation-list><li class="navigation-item align-center"><a class=navigation-link href=https://saswatac.github.io/posts>Blog</a></li><li class="navigation-item align-center"><a class=navigation-link href=https://saswatac.github.io/about>About</a></li></ul></label></section></nav><div class=content><section class="container post"><article><header><h1 class=title>Understanding Event Driven Systems</h1><h2 class=date>June 28, 2019</h2></header><p>(Cross posted, originally on medium (<a href=https://medium.com/@saswatachakravarty/6bc167f59d40>https://medium.com/@saswatachakravarty/6bc167f59d40</a>)</p><p>Event driven system architectures have become become really popular in the last decade. It is at the heart of high performance web servers such as NGINX. The good performance of Node.js can be credited to its asynchronous event driven runtime. There are reactive programming frameworks such as RxJava available today which helps developers build event driven systems. So what is this event driven paradigm and how does it help bring efficiency? This article will help you understand everything about the terms event driven, asynchronous and non blocking I/O using simple python code snippets.</p><p>Consider a typical scenario that is encountered in a micro-services environment— you have some function which makes multiple API requests to downstream services, collects the results and does some manipulation before returning the result to the client. Here is a toy model —</p><p>The downstream service provides an API /get_resource , which has a latency of 1 seconds which we model by letting it sleep for 1 seconds.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>flask</span> <span class=kn>import</span> <span class=n>Flask</span>
<span class=kn>import</span> <span class=nn>time</span>

<span class=n>downstream_service</span> <span class=o>=</span> <span class=n>Flask</span><span class=p>(</span><span class=vm>__name__</span><span class=p>)</span>


<span class=nd>@downstream_service.route</span><span class=p>(</span><span class=s1>&#39;/get_resource&#39;</span><span class=p>)</span>
<span class=k>def</span> <span class=nf>get_resource</span><span class=p>():</span>
    <span class=n>time</span><span class=o>.</span><span class=n>sleep</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
    <span class=k>return</span> <span class=s1>&#39;Hello World!&#39;</span>

<span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
    <span class=n>downstream_service</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>port</span><span class=o>=</span><span class=mi>8080</span><span class=p>)</span>
</code></pre></div><p>The application code needs to make two requests to this API.</p><h2 id=serial-calls>Serial Calls</h2><p>The naive approach is to make two requests one after the other, which costs approximately 2 seconds to execute.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>urllib.request</span>
<span class=kn>import</span> <span class=nn>time</span>


<span class=k>def</span> <span class=nf>call_downstream</span><span class=p>():</span>
    <span class=k>return</span> <span class=n>urllib</span><span class=o>.</span><span class=n>request</span><span class=o>.</span><span class=n>urlopen</span><span class=p>(</span><span class=s1>&#39;http://localhost:8080/get_resource&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>


<span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
<span class=n>call_downstream</span><span class=p>()</span>
<span class=n>call_downstream</span><span class=p>()</span>
<span class=n>end</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
<span class=k>print</span><span class=p>(</span><span class=s2>&#34;Time taken for serial requests:&#34;</span><span class=p>,</span> <span class=n>end</span> <span class=o>-</span> <span class=n>start</span><span class=p>)</span>
</code></pre></div><p>Time taken for serial requests: 2.011678457260132</p><h2 id=using-threadpools>Using ThreadPools</h2><p>The two calls to the downstream service are independent of one another. So instead of waiting 1 seconds for the first request to complete, we can make the requests in parallel. This is typically accomplished using thread pools. We submit the work of calling the downstream service to a background thread managed by a thread pool executor. The main thread does not get blocked, and goes on to submit the next call to the downstream service into the thread pool. It then waits for the result. This approach takes approximately 1 second to execute, as expected.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>time</span>
<span class=kn>import</span> <span class=nn>urllib.request</span>
<span class=kn>from</span> <span class=nn>concurrent.futures</span> <span class=kn>import</span> <span class=n>ThreadPoolExecutor</span>


<span class=k>def</span> <span class=nf>call_downstream</span><span class=p>():</span>
    <span class=k>return</span> <span class=n>urllib</span><span class=o>.</span><span class=n>request</span><span class=o>.</span><span class=n>urlopen</span><span class=p>(</span><span class=s1>&#39;http://localhost:8080/get_resource&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>


<span class=n>executor</span> <span class=o>=</span> <span class=n>ThreadPoolExecutor</span><span class=p>()</span>

<span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
<span class=n>future_1</span> <span class=o>=</span> <span class=n>executor</span><span class=o>.</span><span class=n>submit</span><span class=p>(</span><span class=n>call_downstream</span><span class=p>)</span>
<span class=n>future_2</span> <span class=o>=</span> <span class=n>executor</span><span class=o>.</span><span class=n>submit</span><span class=p>(</span><span class=n>call_downstream</span><span class=p>)</span>
<span class=n>future_1</span><span class=o>.</span><span class=n>result</span><span class=p>()</span>
<span class=n>future_2</span><span class=o>.</span><span class=n>result</span><span class=p>()</span>
<span class=n>end</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
<span class=k>print</span><span class=p>(</span><span class=s2>&#34;Time taken using thread pool:&#34;</span><span class=p>,</span> <span class=n>end</span> <span class=o>-</span> <span class=n>start</span><span class=p>)</span>
</code></pre></div><p>Time taken using thread pool: 1.0046939849853516</p><p>So far, all this is familiar. Now let us ask ourselves the question- can we achieve the same execution time of approximately 1 seconds by making parallel calls using just a single thread?</p><h2 id=using-event-driven-model>Using Event Driven Model</h2><p>It seems counter-intuitive at first — how can you make parallel calls using just a single thread? Event driven models helps us achieve this, as the following sections will make clear. In order to understand this, we must first take a quick peek into how communication takes place over HTTP to the downstream service.</p><p>At a high level, when our application sends a request downstream it first creates an HTTP connection, which is backed by a socket. It then sends the request and then keeps listening on the socket for the response to arrive. Once the response has arrived, it will read it off the socket. All this is encapsulated in the urlopen function call.</p><p>Lets start with implementing the downstream call with the low level http APIs of python.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>http.client</span>

<span class=k>def</span> <span class=nf>call_downstream_low_level</span><span class=p>():</span>
    <span class=n>conn</span> <span class=o>=</span> <span class=n>http</span><span class=o>.</span><span class=n>client</span><span class=o>.</span><span class=n>HTTPConnection</span><span class=p>(</span><span class=s2>&#34;localhost:8080&#34;</span><span class=p>)</span>
    <span class=n>conn</span><span class=o>.</span><span class=n>request</span><span class=p>(</span><span class=s2>&#34;GET&#34;</span><span class=p>,</span> <span class=s2>&#34;/get_resource&#34;</span><span class=p>)</span>
    <span class=n>res</span> <span class=o>=</span> <span class=n>conn</span><span class=o>.</span><span class=n>getresponse</span><span class=p>()</span>
    <span class=k>return</span> <span class=n>res</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</code></pre></div><p>In the function above, the main bottleneck comes from the conn.getresponse() call , which blocks for 1 second waiting for the reply to come from the downstream service. Wouldn’t it be nice if somehow the function gets paused, other useful tasks are performed and then function is resumed only when the response is ready? This is exactly what we are going to achieve. Before making the blocking conn.getresponse() call, we will voluntarily pause and give back the control of execution to the caller of the function, and pass it back the connection socket on which we are waiting to read. This is done with the help of the yield statement.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>http.client</span>
<span class=k>def</span> <span class=nf>call_downstream_async</span><span class=p>():</span>
    <span class=n>conn</span> <span class=o>=</span> <span class=n>http</span><span class=o>.</span><span class=n>client</span><span class=o>.</span><span class=n>HTTPConnection</span><span class=p>(</span><span class=s2>&#34;localhost:8080&#34;</span><span class=p>)</span>
    <span class=n>conn</span><span class=o>.</span><span class=n>request</span><span class=p>(</span><span class=s2>&#34;GET&#34;</span><span class=p>,</span> <span class=s2>&#34;/get_resource&#34;</span><span class=p>)</span>
    <span class=k>yield</span> <span class=n>conn</span><span class=o>.</span><span class=n>sock</span>
    <span class=n>res</span> <span class=o>=</span> <span class=n>conn</span><span class=o>.</span><span class=n>getresponse</span><span class=p>()</span>
    <span class=k>return</span> <span class=n>res</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</code></pre></div><p>Next we will define an event loop which will process the tasks (which in our case is the task of calling downstream) present in a queue, and execute them when they are ready to.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>run</span><span class=p>(</span><span class=n>tasks</span><span class=p>):</span>
    <span class=c1># List of sockets which are waiting for data to arrive</span>
    <span class=c1># along with their corresponding task</span>
    <span class=n>waiting_to_read</span> <span class=o>=</span> <span class=p>{}</span>
    <span class=c1># Main event loop, continue to process events while they exist.</span>
    <span class=k>while</span> <span class=nb>any</span><span class=p>([</span><span class=n>tasks</span><span class=p>,</span> <span class=n>waiting_to_read</span><span class=p>]):</span>
        <span class=c1># If there are no tasks in the queue, check for sockets</span>
        <span class=c1># on which data has arrived.</span>
        <span class=k>while</span> <span class=ow>not</span> <span class=n>tasks</span><span class=p>:</span>
            <span class=n>ready_to_read</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>select</span><span class=o>.</span><span class=n>select</span><span class=p>(</span>
                <span class=n>waiting_to_read</span><span class=p>,</span> <span class=p>[],</span> <span class=p>[])</span>
            <span class=c1># For the sockets on which data is available,</span>
            <span class=c1># remove them from the waiting queue</span>
            <span class=c1># and add the task back to the list.</span>
            <span class=k>for</span> <span class=n>socket</span> <span class=ow>in</span> <span class=n>ready_to_read</span><span class=p>:</span>
                <span class=n>tasks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>waiting_to_read</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=n>socket</span><span class=p>))</span>
        <span class=n>task</span> <span class=o>=</span> <span class=n>tasks</span><span class=o>.</span><span class=n>popleft</span><span class=p>()</span>
        <span class=k>try</span><span class=p>:</span>
            <span class=c1># The socket on which the task is waiting</span>
            <span class=n>sock</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=n>task</span><span class=p>)</span>
            <span class=c1># Add the socket to waiting queue</span>
            <span class=n>waiting_to_read</span><span class=p>[</span><span class=n>sock</span><span class=p>]</span> <span class=o>=</span> <span class=n>task</span>
        <span class=k>except</span> <span class=ne>StopIteration</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
            <span class=k>print</span><span class=p>(</span><span class=s2>&#34;Finished task: &#34;</span><span class=p>,</span> <span class=n>task</span><span class=o>.</span><span class=vm>__name__</span><span class=p>)</span>
            <span class=k>print</span><span class=p>(</span><span class=s2>&#34;Returned value: &#34;</span><span class=p>,</span> <span class=n>e</span><span class=o>.</span><span class=n>value</span><span class=p>)</span>
</code></pre></div><p>Here is how it works. It maintains an internal queue of sockets which are waiting for data to arrive. As long as there are items present in the task queue or the waiting queue, it will continue to process them.</p><p>Our tasks are calls to the function <strong>call_downstream_async()</strong> , which returns a generator. The generator returns a socket on the first next call and exits on the subsequent call. The event loop gets the socket and places them in a wait queue. It checks whether it is ready for read with the help of the select function call, which is delegated to this unix system call. Once it is ready, the task is placed back on the main tasks queue, so that call_downstream_async can resume execution, now that the data is available in the socket to read.</p><p>Finally, in our application code, we create the task queue and call the run function to execute them.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
<span class=n>tasks</span> <span class=o>=</span> <span class=n>deque</span><span class=p>()</span>
<span class=n>tasks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>call_downstream_async</span><span class=p>())</span>
<span class=n>tasks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>call_downstream_async</span><span class=p>())</span>
<span class=n>run</span><span class=p>(</span><span class=n>tasks</span><span class=p>)</span>
<span class=n>end</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
<span class=k>print</span><span class=p>(</span><span class=s2>&#34;Time taken using event loop: &#34;</span><span class=p>,</span> <span class=n>end</span> <span class=o>-</span> <span class=n>start</span><span class=p>)</span>
</code></pre></div><p>Time taken using event loop: 1.0052828788757324</p><p>Thus we see that we are able to achieve execution of 1 seconds using a single thread, with the event driven paradigm.</p><h2 id=event-driven-vs-threads>Event Driven Vs Threads</h2><p>You may ask, what is the benefit of using the event driven model, with its added complexity, over just using a thread pool, given that we achieve the same execution time of 1 seconds?</p><p>Using multiple threads is more expensive that using a single thread. The costs involved in the context switching between the threads adds up. The degree of parallelism depends on the number of threads, which in turn has to be chosen according to the number of available cpu cores.</p><p>Let us compare the timings with 100 requests -</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>executor</span> <span class=o>=</span> <span class=n>ThreadPoolExecutor</span><span class=p>()</span>
<span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
<span class=n>futures</span> <span class=o>=</span> <span class=p>[</span><span class=n>executor</span><span class=o>.</span><span class=n>submit</span><span class=p>(</span><span class=n>call_downstream</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>)]</span>
<span class=n>results</span> <span class=o>=</span> <span class=p>[</span><span class=n>future</span><span class=o>.</span><span class=n>result</span><span class=p>()</span> <span class=k>for</span> <span class=n>future</span> <span class=ow>in</span> <span class=n>futures</span><span class=p>]</span>
<span class=n>end</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
<span class=k>print</span><span class=p>(</span><span class=s2>&#34;Time taken using thread pool:&#34;</span><span class=p>,</span> <span class=n>end</span><span class=o>-</span><span class=n>start</span><span class=p>)</span>
<span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
<span class=n>tasks</span> <span class=o>=</span> <span class=n>deque</span><span class=p>()</span>
<span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>):</span>
    <span class=n>tasks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>call_downstream_async</span><span class=p>())</span>
<span class=n>run</span><span class=p>(</span><span class=n>tasks</span><span class=p>)</span>
<span class=n>end</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
<span class=k>print</span><span class=p>(</span><span class=s2>&#34;Time taken using event loop: &#34;</span><span class=p>,</span> <span class=n>end</span> <span class=o>-</span> <span class=n>start</span><span class=p>)</span>
</code></pre></div><p>Time taken using thread pool: 3.0826776027679443</p><p>Time taken using event loop: 1.0724318027496338</p><p>In this case, using an event loop turns out to be more efficient. The result should be taken with a grain of salt though — the task in these example is completely I/O bound, involving minimum cpu work. However, in real life scenarios the workload will typically have some more compute component. Computational tasks can be parallelized only using multiple threads or processes, so most real life systems will have both event driven asynchronous I/O as well have thread pools or process pools to take advantage of multiple cores of the machine.</p><h2 id=conclusion>Conclusion</h2><p>In the above examples, we helped develop intuition for how event driven systems work and achieve parallelism using a single thread by using constructs like non-blocking calls, voluntarily yielding control (also known as co-operative multitasking) and using a event loop.</p><p>The examples have a lot of scope for improvement — it is not clear how to collect the return values of the asynchronous functions, what happens if there dependencies among the calls to the downstream service, the event loop implementation is task specific; does this mean we need to write our own event loops for our applications? Luckily there are frameworks available which help address these needs. Exploring some of the frameworks will be a topic for a future post.</p><hr><h2 id=acknowledgement>Acknowledgement</h2><p>[1] David Beazley — Python Concurrency From the Ground Up: LIVE! — PyCon 2015</p></article><br></section></div></main><script src=/js/app.js></script></body></html>